
============================= M1 Pro =============================

---- Training a transformers language model ----
=> Values from three executions

Framework: pytorch
	Average: 1806.639595190684s - Median: 1818.6110489368439s

Framework: mlx
	Average: 1157.0066788196564s - Median: 1154.6633532047272s


---- Training BERT ----
=> Values from three executions

Framework: pytorch
	Average: 751.028749704361s - Median: 753.6784768104553s

Framework: mlx
	Average: 718.3554696242014s - Median: 718.211642742157s


---- Whisper inference ----
=> Values from ten executions

Framework: pytorch
	Average: 31.998124384880064s - Median: 31.96485936641693s

Framework: mlx
	Average: 8.509361457824706s - Median: 8.509169936180115s


---- TinyLLama inference ----
=> Values from ten executions

Framework: pytorch
	Average: 59.274635887146s - Median: 55.8025221824646s

Framework: mlx
	Average: 33.38054447174072s - Median: 33.322925329208374s


---- CPU/GPU switch ----
=> Values from ten executions

Framework: pytorch
	Average: 349.7299320459366s - Median: 349.9100536108017s

Framework: mlx
	Average: 270.1572776556015s - Median: 271.8326184749603s

==================================================================

============================= M1 Max =============================

---- Training a transformers language model ----
=> Values from three executions

Framework: pytorch
	Average: 1106.7549295464829s - Median: 1101.3749282211793s

Framework: mlx
	Average: 752.2592077255249s - Median: 752.2592812234227s


---- Training BERT ----
=> Values from three executions

Framework: pytorch
	Average: 793.6759963925679s - Median: 793.610111951828s

Framework: mlx
	Average: 499.343544960022s - Median: 498.0613958835602s


---- Whisper inference ----
=> Values from ten executions

Framework: pytorch 
	Average: 21.27653947197935s - Median: 21.204530119895964s

Framework: mlx
	Average: 6.946336317062378s - Median: 6.937196493148804s


---- TinyLLama inference ----
=> Values from ten executions

Framework: pytorch
	Average: 50.98743650913239s - Median: 47.73779845237732s

Framework: mlx
	Average: 20.61291913986206s - Median: 20.613165140151978s


---- CPU/GPU switch ----
=> Values from ten executions

Framework: pytorch
	Average: 251.71098244190216s - Median: 252.021404504776s

Framework: mlx
	Average: 214.5735635280609s - Median: 214.57463908195496s

==================================================================

============================= M3 Max =============================

---- Training a transformers language model ----
=> Values from three executions

Framework: pytorch
	Average: 912.5205717086792s - Median: 924.3736660480499s

Framework: mlx
	Average: 426.00355768203735s - Median: 426.1944200992584s


---- Training BERT ----
=> Values from three executions

Framework: pytorch
	Average: 550.2911033630371s - Median: 544.6988077163696s

Framework: mlx
	Average: 408.45258100827533s - Median: 408.62774896621704s


---- Whisper inference ----
=> Values from ten executions

Framework: pytorch
	Average: 17.909158730506896s - Median: 17.877812027931213s

Framework: mlx
	Average: 4.8507798433303835s - Median: 4.839159846305847s


---- TinyLLama inference ----
=> Values from ten executions

Framework: pytorch
	Average: 36.182030129432675s - Median: 34.26037609577179s

Framework: mlx
	Average: 15.41469841003418s - Median: 15.389396786689758s


---- CPU/GPU switch ----
=> Values from ten executions

Framework: pytorch
	Average: 146.35703275203704s - Median: 146.41792500019073s

Framework: mlx
	Average: 140.5102721452713s - Median: 140.51127195358276s

==================================================================
